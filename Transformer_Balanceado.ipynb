{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer PT→EN — versão **rápida e balanceada** (GPU-only)\n",
        "\n",
        "Este notebook é **inspirado no tutorial oficial do TensorFlow** de tradução com Transformer, porém foi **ajustado para um melhor equilíbrio entre tempo de execução e qualidade**. A ideia é você **rodar tudo no Colab com GPU**, ver resultados rapidamente e ainda ter “alavancas” simples para melhorar a qualidade quando quiser.\n",
        "\n",
        "## O que muda em relação ao tutorial\n",
        "- **Subset do dataset (TFDS)** para acelerar o treinamento inicial.\n",
        "- **Modelo menor**, porém não minúsculo (menos camadas e dimensões), mantendo a essência do Transformer (encoder–decoder, atenção multi-cabeças e positional encoding).\n",
        "- **Hiperparâmetros acessíveis** (épocas, tamanho do lote, número de tokens, tamanho do modelo) concentrados em uma célula para você ajustar facilmente.\n",
        "- **Treino apenas na GPU** (GPU-only), com opção de **mixed precision** para ganhar desempenho quando suportado.\n",
        "- **Inferência com beam search** opcional (melhora a tradução sem precisar re-treinar).\n",
        "\n",
        "> Objetivo: viabilizar uma experiência prática e repetível do Transformer em PT→EN, com um ponto de partida rápido e espaço para escalar qualidade conforme a necessidade."
      ],
      "metadata": {
        "id": "dmADz--cAf5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 01 — Deps e GPU\n",
        "%pip -q install -U \"protobuf~=3.20.3\" tensorflow \"tensorflow-text\" tensorflow_datasets\n",
        "\n",
        "import tensorflow as tf, sys\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"TF:\", tf.__version__)\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "assert gpus, \"Ative o runtime GPU (Runtime > Change runtime type > GPU).\"\n",
        "print(\"GPU detectada:\", gpus)"
      ],
      "metadata": {
        "id": "5dHUVCsp-IwB",
        "outputId": "02bd8589-ecf5-46a6-b52c-4894fe5947a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "TF: 2.19.1\n",
            "GPU detectada: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 02 — Imports e configs (melhor qualidade)\n",
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np, time, json, pathlib\n",
        "\n",
        "MAX_TOKENS   = 128    # ↑ menos truncamento\n",
        "BATCH_SIZE   = 64\n",
        "EPOCHS       = 8      # ↑ mais treino\n",
        "\n",
        "TRAIN_TAKE   = 30000  # ↑ mais dados (ajuste se faltar VRAM)\n",
        "VAL_TAKE     = 1500\n",
        "\n",
        "# Modelo médio\n",
        "NUM_LAYERS   = 4\n",
        "D_MODEL      = 128\n",
        "DFF          = 512\n",
        "NUM_HEADS    = 8\n",
        "DROPOUT      = 0.1"
      ],
      "metadata": {
        "id": "tlRwiafJ-It5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 03 — Carregar dataset e fazer subset\n",
        "examples, info = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']\n",
        "\n",
        "train_examples = train_examples.take(TRAIN_TAKE)\n",
        "val_examples   = val_examples.take(VAL_TAKE)\n",
        "\n",
        "for pt, en in train_examples.take(2):\n",
        "    print(\"PT:\", pt.numpy().decode())\n",
        "    print(\"EN:\", en.numpy().decode(), \"\\n\")"
      ],
      "metadata": {
        "id": "RCKOIUIV-Ir0",
        "outputId": "674a69fa-370e-442a-f2ce-b78ead1313a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PT: e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "EN: and when you improve searchability , you actually take away the one advantage of print , which is serendipity . \n",
            "\n",
            "PT: mas e se estes fatores fossem ativos ?\n",
            "EN: but what if it were active ? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 04 — Tokenizers prontos (SavedModel)\n",
        "import zipfile, pathlib\n",
        "\n",
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "zip_path = tf.keras.utils.get_file(\n",
        "    fname=f'{model_name}.zip',\n",
        "    origin=f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
        "    cache_dir='/content', cache_subdir='', extract=False\n",
        ")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "    zf.extractall('/content')\n",
        "\n",
        "cands = list(pathlib.Path('/content').rglob('saved_model.pb'))\n",
        "assert cands, \"saved_model.pb não encontrado após extração.\"\n",
        "tokenizers = tf.saved_model.load(str(cands[0].parent))\n",
        "print(\"OK tokenizers. Vocabs:\", int(tokenizers.pt.get_vocab_size().numpy()), int(tokenizers.en.get_vocab_size().numpy()))"
      ],
      "metadata": {
        "id": "m-SngtjY-Ipe",
        "outputId": "e7e5f34d-9d7e-42c6-db33-0b561647344c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK tokenizers. Vocabs: 7765 7010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 05 — Pipeline tf.data\n",
        "def prepare_batch(pt, en):\n",
        "    pt = tokenizers.pt.tokenize(pt)[:, :MAX_TOKENS].to_tensor()\n",
        "    en = tokenizers.en.tokenize(en)[:, :(MAX_TOKENS+1)]\n",
        "    en_in  = en[:, :-1].to_tensor()\n",
        "    en_lbl = en[:,  1:].to_tensor()\n",
        "    return (pt, en_in), en_lbl\n",
        "\n",
        "def make_batches(ds, training=True):\n",
        "    if training: ds = ds.shuffle(2048, reshuffle_each_iteration=True)\n",
        "    return (ds.batch(BATCH_SIZE)\n",
        "              .map(prepare_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "              .cache()\n",
        "              .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "train_batches = make_batches(train_examples, True)\n",
        "val_batches   = make_batches(val_examples,   False)\n",
        "\n",
        "print(next(iter(train_batches))[0][0].shape)  # sanity: (batch, len)"
      ],
      "metadata": {
        "id": "vy6M3PuW-InJ",
        "outputId": "6555cd29-4339-4cf8-c3c1-513afa02d336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 06 — Camadas essenciais (mínimas)\n",
        "def positional_encoding(length, depth):\n",
        "    depth = depth/2\n",
        "    positions = np.arange(length)[:, np.newaxis]\n",
        "    depths = np.arange(depth)[np.newaxis, :]/depth\n",
        "    angle_rates = 1/(10000**depths)\n",
        "    angle_rads = positions * angle_rates\n",
        "    pe = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "    return tf.cast(pe, tf.float32)\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "        self.pos_encoding = positional_encoding(2048, d_model)\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.embedding.compute_mask(*args, **kwargs)\n",
        "    def call(self, x):\n",
        "        length = tf.shape(x)[1]\n",
        "        x = self.embedding(x) * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        return x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, key_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=dropout)\n",
        "        self.add = tf.keras.layers.Add()\n",
        "        self.norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "    def call(self, x):\n",
        "        attn = self.mha(x, x, use_causal_mask=False)\n",
        "        return self.norm(self.add([x, attn]))\n",
        "\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "    def call(self, x):\n",
        "        attn = self.mha(x, x, use_causal_mask=True)\n",
        "        return self.norm(self.add([x, attn]))\n",
        "\n",
        "class CrossAttention(BaseAttention):\n",
        "    def call(self, x, context):\n",
        "        attn = self.mha(x, context)\n",
        "        return self.norm(self.add([x, attn]))\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, dff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.seq = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model),\n",
        "            tf.keras.layers.Dropout(dropout),\n",
        "        ])\n",
        "        self.add = tf.keras.layers.Add()\n",
        "        self.norm = tf.keras.layers.LayerNormalization()\n",
        "    def call(self, x):\n",
        "        return self.norm(self.add([x, self.seq(x)]))"
      ],
      "metadata": {
        "id": "csbolzwv-IlD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 07 — Modelo mínimo\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, drop):\n",
        "        super().__init__()\n",
        "        self.sa = GlobalSelfAttention(num_heads, d_model, drop)\n",
        "        self.ff = FeedForward(d_model, dff, drop)\n",
        "    def call(self, x):\n",
        "        x = self.sa(x); x = self.ff(x); return x\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, drop):\n",
        "        super().__init__()\n",
        "        self.ca = CausalSelfAttention(num_heads, d_model, drop)\n",
        "        self.xa = CrossAttention(num_heads, d_model, drop)\n",
        "        self.ff = FeedForward(d_model, dff, drop)\n",
        "    def call(self, x, context):\n",
        "        x = self.ca(x); x = self.xa(x, context); x = self.ff(x); return x\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, n, d_model, num_heads, dff, vocab, drop):\n",
        "        super().__init__()\n",
        "        self.pe = PositionalEmbedding(vocab, d_model)\n",
        "        self.drop = tf.keras.layers.Dropout(drop)\n",
        "        self.layers = [EncoderLayer(d_model, num_heads, dff, drop) for _ in range(n)]\n",
        "    def call(self, x):\n",
        "        x = self.drop(self.pe(x))\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, n, d_model, num_heads, dff, vocab, drop):\n",
        "        super().__init__()\n",
        "        self.pe = PositionalEmbedding(vocab, d_model)\n",
        "        self.drop = tf.keras.layers.Dropout(drop)\n",
        "        self.layers = [DecoderLayer(d_model, num_heads, dff, drop) for _ in range(n)]\n",
        "    def call(self, x, context):\n",
        "        x = self.drop(self.pe(x))\n",
        "        for l in self.layers: x = l(x, context)\n",
        "        return x\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, *, n, d_model, num_heads, dff, in_vocab, tgt_vocab, drop):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(n, d_model, num_heads, dff, in_vocab, drop)\n",
        "        self.decoder = Decoder(n, d_model, num_heads, dff, tgt_vocab, drop)\n",
        "        self.final = tf.keras.layers.Dense(tgt_vocab)\n",
        "    def call(self, inputs):\n",
        "        context, x = inputs\n",
        "        c = self.encoder(context)\n",
        "        x = self.decoder(x, c)\n",
        "        logits = self.final(x)\n",
        "        try: del logits._keras_mask\n",
        "        except: pass\n",
        "        return logits"
      ],
      "metadata": {
        "id": "x6M4uWLd-Iit"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 08 — Treino rápido na GPU\n",
        "gpu_strategy = tf.distribute.OneDeviceStrategy(\"/GPU:0\")\n",
        "with gpu_strategy.scope():\n",
        "    transformer = Transformer(\n",
        "        n=NUM_LAYERS, d_model=D_MODEL, num_heads=NUM_HEADS, dff=DFF,\n",
        "        in_vocab=int(tokenizers.pt.get_vocab_size().numpy()),\n",
        "        tgt_vocab=int(tokenizers.en.get_vocab_size().numpy()),\n",
        "        drop=DROPOUT\n",
        "    )\n",
        "    # \"tocar\" o modelo\n",
        "    (pt_b, en_in_b), en_lbl_b = next(iter(train_batches))\n",
        "    _ = transformer((pt_b, en_in_b))\n",
        "\n",
        "    class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "        def __init__(self, d_model, warmup_steps=2000):  # warmup menor\n",
        "            super().__init__()\n",
        "            self.d_model = tf.cast(d_model, tf.float32)\n",
        "            self.warmup_steps = warmup_steps\n",
        "        def __call__(self, step):\n",
        "            step = tf.cast(step, tf.float32)\n",
        "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(\n",
        "                tf.math.rsqrt(step), step * (self.warmup_steps ** -1.5)\n",
        "            )\n",
        "\n",
        "    def masked_loss(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred)\n",
        "        mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "        return tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)\n",
        "\n",
        "    def masked_accuracy(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.int64)\n",
        "        y_pred = tf.argmax(y_pred, axis=-1)\n",
        "        mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "        match = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n",
        "        return tf.reduce_sum(match * mask) / tf.reduce_sum(mask)\n",
        "\n",
        "    lr = CustomSchedule(D_MODEL)\n",
        "    opt = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "    transformer.compile(optimizer=opt, loss=masked_loss, metrics=[masked_accuracy])\n",
        "\n",
        "t0 = time.time()\n",
        "hist = transformer.fit(train_batches, epochs=EPOCHS, validation_data=val_batches, verbose=1)\n",
        "t1 = time.time()\n",
        "print(f\"Tempo total: {t1 - t0:.1f}s\")"
      ],
      "metadata": {
        "id": "yYRPBnwA-IgX",
        "outputId": "c23b22ca-5dd0-4f01-d8b8-a1e037f3be81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'global_self_attention_2' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'encoder_layer_2' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'causal_self_attention_2' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'decoder_layer_2' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 237ms/step - loss: 7.7166 - masked_accuracy: 0.0854 - val_loss: 5.0514 - val_masked_accuracy: 0.2476\n",
            "Epoch 2/8\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 230ms/step - loss: 4.7768 - masked_accuracy: 0.2766 - val_loss: 4.1437 - val_masked_accuracy: 0.3517\n",
            "Epoch 3/8\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 230ms/step - loss: 3.9646 - masked_accuracy: 0.3657 - val_loss: 3.6169 - val_masked_accuracy: 0.4076\n",
            "Epoch 4/8\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 230ms/step - loss: 3.4340 - masked_accuracy: 0.4198 - val_loss: 3.3371 - val_masked_accuracy: 0.4400\n",
            "Epoch 5/8\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 230ms/step - loss: 3.0383 - masked_accuracy: 0.4613 - val_loss: 3.0551 - val_masked_accuracy: 0.4822\n",
            "Epoch 6/8\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 230ms/step - loss: 2.5841 - masked_accuracy: 0.5158 - val_loss: 2.9313 - val_masked_accuracy: 0.4963\n",
            "Epoch 7/8\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 230ms/step - loss: 2.2349 - masked_accuracy: 0.5600 - val_loss: 2.8424 - val_masked_accuracy: 0.5068\n",
            "Epoch 8/8\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 229ms/step - loss: 1.9675 - masked_accuracy: 0.5962 - val_loss: 2.8681 - val_masked_accuracy: 0.5104\n",
            "Tempo total: 959.0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 09 — Tradução greedy simples\n",
        "class Translator(tf.Module):\n",
        "    def __init__(self, tokenizers, transformer, max_len=MAX_TOKENS):\n",
        "        self.tk = tokenizers\n",
        "        self.m = transformer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "    def __call__(self, sentence):\n",
        "        if tf.rank(sentence) == 0: sentence = sentence[tf.newaxis]\n",
        "        enc_in = self.tk.pt.tokenize(sentence).to_tensor()\n",
        "        se = self.tk.en.tokenize([''])[0]\n",
        "        start, end = se[0][tf.newaxis], se[1][tf.newaxis]\n",
        "        out = tf.TensorArray(tf.int64, size=0, dynamic_size=True).write(0, start)\n",
        "        for _ in tf.range(self.max_len):\n",
        "            dec = tf.transpose(out.stack())\n",
        "            logits = self.m([enc_in, dec], training=False)[:, -1:, :]\n",
        "            next_id = tf.argmax(logits, axis=-1)\n",
        "            out = out.write(out.size(), next_id[0])\n",
        "            if tf.reduce_all(tf.equal(next_id, end)): break\n",
        "        tokens = tf.transpose(out.stack())\n",
        "        text = self.tk.en.detokenize(tokens)[0]\n",
        "        return text\n",
        "\n",
        "translator = Translator(tokenizers, transformer)\n",
        "\n",
        "def demo(s, ref=None):\n",
        "    pred = translator(tf.constant(s)).numpy().decode()\n",
        "    print(\"PT:\", s)\n",
        "    print(\"EN:\", pred)\n",
        "    if ref: print(\"REF:\", ref)\n",
        "    print()\n",
        "\n",
        "demo(\"este é um problema que temos que resolver.\", \"this is a problem we have to solve .\")\n",
        "demo(\"os meus vizinhos ouviram sobre esta ideia.\", \"and my neighboring homes heard about this idea .\")\n",
        "demo(\"estou aprendendo um modelo transformer pequeno.\")"
      ],
      "metadata": {
        "id": "jCEHC3Eb-IeC",
        "outputId": "83a6cb53-a320-4e3e-f685-3cca7b580032",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PT: este é um problema que temos que resolver.\n",
            "EN: this is a problem that we have to solve . we have to solve .\n",
            "REF: this is a problem we have to solve .\n",
            "\n",
            "PT: os meus vizinhos ouviram sobre esta ideia.\n",
            "EN: my neighbors had heard this idea about this idea .\n",
            "REF: and my neighboring homes heard about this idea .\n",
            "\n",
            "PT: estou aprendendo um modelo transformer pequeno.\n",
            "EN: i ' m studying a little model , i ' m transformed with small model .\n",
            "\n"
          ]
        }
      ]
    }
  ]
}